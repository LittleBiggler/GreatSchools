{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eccde744-aa2e-4db0-9c00-0ec3f3433bd8",
   "metadata": {},
   "source": [
    "# Project Description\n",
    "\n",
    "**The Question**: Which features of public schools predict above-average school ranking?\n",
    "\n",
    "This project uses data that I purchased and loaded from the GreatSchools.org API in 2021. The API endpoint for school rankings and demographics data was defined by a radius of geographical distance from Mount Kisco, NY, where I lived at the time. \n",
    "\n",
    "In all likelihood, I will purchase more data from GreatSchools to perform a 4-year comparison.\n",
    "\n",
    "\n",
    "**What the GreatSchools Rating measures:**\n",
    "\n",
    "The GreatSchools Rating is a 1–10 score, purchased by real estate listing aggregators like Zillow. Ratings at the lower end of the scale (1–4) signal below-average performance, 5–6 indicate average performance, and 7–10 demonstrate above-average performance.\n",
    "\n",
    "The GreatSchools Rating is based on up to three themed ratings, which are designed to capture different aspects of school quality:\n",
    "\n",
    "1) Student Progress Rating: Calculated using state-reported student growth data. If unavailable, it is replaced with the Academic Progress Rating, a proxy growth measure GreatSchools creates when sufficient state-produced student growth data is not available.\n",
    "2) College Readiness Rating: A multi-measure rating based on college entrance exams (SAT and ACT), high school graduation rates, and advanced coursework participation (Advanced Placement (AP), International Baccalaureate (IB), or dual enrollment).\n",
    "3) Test Score Rating: Based on state-standardized test performance.\n",
    "\n",
    "GreatSchools calculate the 1-10 Rating using a two-step weighted average approach.\n",
    "1) First, they assign base weights to each themed rating based on the strength of research linking that data to long-term student outcomes and our mission to highlight schools that support academic growth for all students. \n",
    "2) Second, they apply information weights that reflect the quantity and variability of data available for each themed rating. To maintain balance, these weights are capped so that no themed rating can outweigh the Student Progress Rating (or the Academic Progress Rating if growth data is not available). If any themed ratings are missing, the remaining weights are rebalanced to sum to one.\n",
    "\n",
    "For more detail, see: https://www.greatschools.org/gk/about/ratings-methodology/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c9b7dc-3ce2-48e4-b010-e9db2c8ab147",
   "metadata": {},
   "source": [
    "# Imports Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2fbc33f-043a-4a7f-9294-956cc52dc73a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4e\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 7\u001b[0m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mset_option(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdisplay.float_format\u001b[39m\u001b[38;5;124m'\u001b[39m, smart_num_format)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Exponential notation that preserves float up to 4 deep\n",
    "def smart_num_format(x):\n",
    "    if abs(x) < 9_999_999:\n",
    "        return f\"{x:.4f}\"\n",
    "    else:\n",
    "        return f\"{x:.4e}\"\n",
    "pd.set_option('display.float_format', smart_num_format)\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, train_test_split\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin # for ILR transform\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import (\n",
    "    roc_curve, auc,\n",
    "    precision_recall_curve,\n",
    "    average_precision_score\n",
    ")\n",
    "from sklearn.calibration import calibration_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25da037-a4dd-41a4-9475-96cf4fe3da19",
   "metadata": {},
   "source": [
    "# EDA\n",
    "\n",
    "Data consists of two tables, which I constructed by parsing the JSON emitted from the GreatSchools.org API.\n",
    "\n",
    "1) schools_raw - Describes location, grade level, rating, and distance to origin\n",
    "- 2,349 schools\n",
    "- id (joined to Universal_ID in demographics)\n",
    "- 13 feature columns\n",
    "\n",
    "\n",
    "\n",
    "2) demog_raw - Describes enrollment, teacher salaries, student-teacher ratio, socioeconomic class markers, racial composition\n",
    "- 1,598 schools\n",
    "- Universal_ID\n",
    "- 18 feature columns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaeb7004-5a4b-4cea-9250-82c57ecac752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in schools raw data\n",
    "schools_raw = pd.read_csv('schools_data_BACKUP.csv')\n",
    "schools = schools_raw.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ce80d0-7c65-451e-aab5-3c9ca81dbe12",
   "metadata": {},
   "outputs": [],
   "source": [
    "schools.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebb0553-ee5b-4db0-9d8c-465ed5eeaed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "schools.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43cf5de-f552-423f-8975-d94a7e77acc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in demographics raw data\n",
    "demog_raw = pd.read_csv('demographics_mstr_BACKUP.csv')\n",
    "demog = demog_raw.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded2ad22-9955-4e58-bc7c-7ea3b044c233",
   "metadata": {},
   "outputs": [],
   "source": [
    "demog.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d2b443-db86-4339-9419-cacd02707091",
   "metadata": {},
   "outputs": [],
   "source": [
    "demog.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f5491a-7ee0-4ba6-b0c8-47ce142113f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge right on 'id', left on 'Universal_ID'\n",
    "\n",
    "df = schools.merge(demog, left_on='id', right_on='Universal_ID', how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d36dc14-0a13-483c-84ab-13baba86c3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect merge results\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db00a851-b7e0-4fed-88d2-da47d0826cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# look for big patterns\n",
    "\n",
    "df[df.columns.difference(['id','rating_year', 'Universal_ID'])].describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa79d11f-0f0e-4617-8d23-092da82ea4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# observe distribution of school ratings\n",
    "\n",
    "df['rating'].hist()\n",
    "plt.title('Distribution of School Ratings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379d5cac-7ec9-4ff1-8c1b-cf58ec5a1789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for duplicates in main identifier\n",
    "\n",
    "df[df.duplicated(subset='id', keep=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3eb988-4cb4-48c6-a8f4-3f7793d80a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# schools per district\n",
    "\n",
    "df['district'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55be174c-5376-48a0-8db0-63db73aa8e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean rating by district\n",
    "\n",
    "(\n",
    "    df.groupby(by='district')['rating']\n",
    "    .agg(mean='mean').sort_values(by='mean', ascending=False).head(50)\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b680b1ac-1908-42a4-9bad-a1263fba38e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# missingness\n",
    "\n",
    "sns.heatmap(df.isnull())\n",
    "plt.title('Missingness')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56afb94f-7a8d-4726-8aa3-359b103ad84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# percent null\n",
    "\n",
    "(df.isnull().sum()/len(df)*100).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8161c31f-b262-4bfd-9b14-ec012da8eaf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop 100% null column\n",
    "\n",
    "df = df.drop(columns=['percent-disadvantaged'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6f7d14-99a4-45c2-a269-be842979d762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross join on district to fill average salary?\n",
    "# check if each school district has one unique \"average salary\"\n",
    "\n",
    "unique_salary = (\n",
    "    df.groupby(by='district')['average-salary']\n",
    "    .agg(nunique='nunique').sort_values(by='nunique')['nunique'].eq(1)\n",
    ")\n",
    "\n",
    "unique_salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b48e498-9f9f-48f2-bc5f-ea9825ecf470",
   "metadata": {},
   "outputs": [],
   "source": [
    "#inspect salaries per district\n",
    "df.dropna(subset='district').set_index(['district','average-salary']).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8a575a-5e54-4682-9594-1a83244c4192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# observe asian and 'Asian or Pacific Islander'\n",
    "\n",
    "df[['Asian', 'Asian or Pacific Islander']] # appears to be an or-condition\n",
    "\n",
    "bad_overlap = (df['Asian'].notna() & df['Asian or Pacific Islander'].notna())\n",
    "\n",
    "assert not bad_overlap.any(), 'Columns not mergable in current form'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fabcfd-8e8c-4d08-8909-cc7f136fa1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge asian and asian or pacific islander\n",
    "\n",
    "df['Asian_combo'] = (\n",
    "    df['Asian'].fillna(df['Asian or Pacific Islander'])\n",
    ")\n",
    "\n",
    "# drop original cols\n",
    "\n",
    "df = df.drop(columns=['Asian', 'Asian or Pacific Islander'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba12e93-9479-41f9-a3ac-de508cb72741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect feature correlations with heatmap\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16,16))\n",
    "corr = df.drop(\n",
    "    columns=['Universal_ID','id','zip_code','distance', 'rating_year','latitude', 'longitude']\n",
    ").select_dtypes(include='number').corr()\n",
    "sns.heatmap(corr, annot=True, cbar=False, cmap='coolwarm')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330c6800-7bd1-4481-846d-bae09d6b0004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop data with no rating\n",
    "\n",
    "df_rating = df.dropna(subset='rating').copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738e0aa8-0be7-4db6-919d-07a66c879059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check missingness after dropping blank ratings\n",
    "\n",
    "sns.heatmap(df_rating.isna(), cbar=False)\n",
    "plt.title('Missingness After Dropna on Rating')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88db5fc-7e19-4673-be7d-6ffaeb1f2a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#observe remaining missingness\n",
    "\n",
    "(df_rating.isnull().sum()/len(df_rating)*100).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e2f47e-253c-4ed9-ba4a-cd1c05d6dc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stacked bar chart\n",
    "\n",
    "demog_race = ['Hispanic', 'White','African American', 'Asian_combo', \n",
    "              'Native Hawaiian or Other Pacific Islander', 'Two or more races',\n",
    "              'Native American']\n",
    "\n",
    "plot_df = (\n",
    "    df_rating\n",
    "    .groupby('rating')[demog_race]\n",
    "    .mean()     # or .sum(), depending on meaning\n",
    "    .sort_index()\n",
    ")\n",
    "\n",
    "ax = plot_df.plot(kind='bar', stacked=True, figsize=(8, 5))\n",
    "ax.set_ylabel('Share')\n",
    "ax.set_xlabel('Rating')\n",
    "ax.legend(title='Race', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.title(\"NYC Metro Demographic Composition by Public School Ranking\")\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff36886b-1c8e-4d23-9004-859b3f220e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model funding-related features\n",
    "\n",
    "\n",
    "bar_cols = ['percent-reduced-lunch', \n",
    "'student-counselor-ratio', \n",
    "'teachers-experience', \n",
    "'percentage-certified', \n",
    "'average-salary',\n",
    "           'student-teacher-ratio']\n",
    "\n",
    "for col in bar_cols:\n",
    "    plot_table = (df_rating.groupby('rating')[col]\n",
    "    .agg(col='mean')\n",
    "    .sort_index())\n",
    "    \n",
    "    plot_table.plot(kind='bar', legend=False)\n",
    "    plt.title(f'{col.title().replace(\"-\",\" \")} by Rating')\n",
    "    plt.ylabel(f\"{col}\".replace(\"-\",\" \"))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6348ae8-9780-45df-a3ea-d5699ea8b32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check_for_outliers\n",
    "\n",
    "scaler = StandardScaler() #apples to apples\n",
    "scaled_array = scaler.fit_transform(df_rating.select_dtypes(include=['number']).copy())\n",
    "df_scaled = pd.DataFrame(scaled_array,\n",
    "                        columns=df_rating.select_dtypes(include=['number']).columns, \n",
    "                         index=df_rating.index)\n",
    "\n",
    "df_scaled\n",
    "sns.boxplot(df_scaled, orient='h')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8b8d60-2802-4961-a0ba-4748767297d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define simplex cols\n",
    "\n",
    "simplex_cols = ['Hispanic', 'White',\n",
    "       'African American', 'Native Hawaiian or Other Pacific Islander',\n",
    "       'Two or more races', 'Native American', 'Asian_combo']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04bd9ff-2c19-4053-9691-e0ea2c8c57c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if any cols add to 100\n",
    "\n",
    "#assert np.isclose(df_rating[simplex_cols].sum(axis=1), 100).all(), \"Simplexes not all mathing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b738169-94b2-4459-a7b5-8a1c1dac5ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rows where simplex columns are working\n",
    "\n",
    "sns.heatmap(df_rating[np.isclose(df_rating[simplex_cols].sum(axis=1), 100, atol=0.01)].isna(), cbar=False)\n",
    "plt.title('Missingness Where Simplex Cols Add to One')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1084b9b5-ce87-4df7-ae4a-c2e5ac67b182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# observe rows where simplexes do not add up\n",
    "\n",
    "df_rating[~np.isclose(df_rating[['Hispanic', 'White',\n",
    "       'African American', 'Native Hawaiian or Other Pacific Islander',\n",
    "       'Two or more races', 'Native American', 'Asian_combo']].sum(axis=1), 100, atol=0.01)]['state'].value_counts()\n",
    "\n",
    "# all rows are in the state of Connecticut, which maybe does not publish demographic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268fff11-8440-4a09-820c-2dea6eb36d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows where state is connecticut\n",
    "# connecticut data could be useful in some other context\n",
    "\n",
    "ct_rows = df_rating[df_rating['state']=='CT'].index\n",
    "\n",
    "df_rating = df_rating.drop(ct_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e7d1cf-e2cb-4f53-9957-b2e0c1037026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify that simplex columns now add all to one with Connecticut dropped\n",
    "assert np.isclose(df_rating[['Hispanic', 'White',\n",
    "       'African American', 'Native Hawaiian or Other Pacific Islander',\n",
    "       'Two or more races', 'Native American', 'Asian_combo']].sum(axis=1), 100).all(), \"Simplexes not mathing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbbc6ca-fe8d-4b99-8af1-3a04f9375746",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill null values in add-to-one columns with zeros\n",
    "# if the columns add to one, the nulls represent zeros\n",
    "\n",
    "df_rating[simplex_cols] = df_rating[simplex_cols].fillna(0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e17ed1-959b-46d6-aa6d-319ffce627b9",
   "metadata": {},
   "source": [
    "## Changes Log\n",
    "\n",
    "Dropped:\n",
    "1) The \"percent-disadvantaged\" column, which was 100% blank\n",
    "2) Rows (schools) where \"rating\" column was blank\n",
    "3) All rows for the state of Connecticut, which lacked demographic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e93e7b-d071-400b-be9f-1a5a3b2e9582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check missingness again\n",
    "sns.heatmap(df_rating.isnull())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5082fb5-65f4-44ae-bf09-cd7d7cb48ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# observe nulls\n",
    "# could these be zeros in disguise?\n",
    "nulls_cols = df_rating.columns[df_rating.isnull().sum()>0]\n",
    "\n",
    "df_rating[nulls_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab000c2-9c68-431e-ac3f-3f3c3a2b565b",
   "metadata": {},
   "source": [
    "# Pipeline\n",
    "\n",
    "I use sklearn Pipeline to manage preprocessing, modelling, cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b830c1d-2425-44c7-85ab-3a2d08fd2560",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct a binary target\n",
    "mean_rating = df_rating['rating'].mean()\n",
    "\n",
    "df_rating['above_average'] = np.where(df_rating['rating']>mean_rating, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0ea156-852b-43e2-859c-7a3a466b51d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# condense zip codes into smaller categories\n",
    "\n",
    "df_rating['zip_4'] = df_rating['zip_code'].astype('str').str[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f88f62c-4467-4bed-914e-fed73317906e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create ILR transformer\n",
    "class ILRTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Apply ILR transform to a set of compositional columns.\n",
    "    X is expected to be a DataFrame when used standalone;\n",
    "    inside ColumnTransformer it will be a NumPy array with only `cols`.\n",
    "    \"\"\"\n",
    "    def __init__(self, cols, prefix=\"ilr\"):\n",
    "        self.cols = cols\n",
    "        self.prefix = prefix\n",
    "        self.V_ = None   # ILR basis (D-1 x D)\n",
    "        self.feature_names_out_ = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        D = len(self.cols)\n",
    "        # Build a Helmert-like matrix to get an orthonormal basis\n",
    "        H = np.zeros((D, D))\n",
    "        for i in range(1, D):\n",
    "            H[i, :i] = 1.0 / i\n",
    "            H[i, i] = -1.0\n",
    "        # QR on the (D x (D-1)) submatrix to get orthonormal columns\n",
    "        Q, _ = np.linalg.qr(H[:, 1:])\n",
    "        self.V_ = Q.T      # shape: (D-1, D)\n",
    "\n",
    "        # tell sklearn how many features we output and what they’re called\n",
    "        n_out = D - 1\n",
    "        self.feature_names_out_ = np.array(\n",
    "            [f\"{self.prefix}_{j}\" for j in range(n_out)],\n",
    "            dtype=object\n",
    "        )\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # handle both DataFrame and ndarray (inside ColumnTransformer)\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            Xs = X[self.cols].to_numpy(dtype=float)\n",
    "        else:\n",
    "            Xs = np.asarray(X, dtype=float)\n",
    "\n",
    "        # Avoid log(0)\n",
    "        Xs = np.clip(Xs, 1e-12, None)\n",
    "        logX = np.log(Xs)\n",
    "        clr = logX - logX.mean(axis=1, keepdims=True)  # n x D\n",
    "        # ILR coords: n x (D-1)\n",
    "        Z = clr @ self.V_.T\n",
    "        return Z\n",
    "\n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        # ColumnTransformer will pass in the original feature names subset;\n",
    "        # we just ignore and return our ILR coord names.\n",
    "        return self.feature_names_out_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45937d4e-da33-46e8-b685-e6c7af50e0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline\n",
    "\n",
    "# define the columns\n",
    "simplex_cols = simplex_cols\n",
    "\n",
    "num_cols     = ['distance', 'enrollment', 'percent-reduced-lunch',\n",
    "       'percent-limited-english', 'average-salary', 'student-teacher-ratio',\n",
    "       'student-counselor-ratio', 'percentage-female', 'teachers-experience',\n",
    "       'percentage-certified']   \n",
    "\n",
    "cat_cols     = [\"zip_4\", 'state']   # not many states represented in the data\n",
    "\n",
    "\n",
    "# numeric pipeline: impute + scale\n",
    "\n",
    "cat_pre = Pipeline([\n",
    "    (\"encode\", OneHotEncoder(handle_unknown='ignore'))\n",
    "    \n",
    "])\n",
    "\n",
    "num_pre = Pipeline([\n",
    "    (\"impute\", SimpleImputer(strategy=\"mean\", add_indicator=True)),\n",
    "    (\"scale\", StandardScaler()),\n",
    "])\n",
    "\n",
    "ilr_pre = Pipeline([\n",
    "    (\"ilr\", ILRTransformer(simplex_cols)),\n",
    "    (\"scale\", StandardScaler())\n",
    "])\n",
    "\n",
    "\n",
    "logit_preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", num_pre, num_cols),\n",
    "        (\"cat\", cat_pre, cat_cols),\n",
    "        (\"ilr\", ilr_pre, simplex_cols)\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    ")\n",
    "\n",
    "tree_preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", num_pre, num_cols),\n",
    "        (\"cat\", cat_pre, cat_cols),\n",
    "        (\"simplex\", \"passthrough\", simplex_cols),\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    ")\n",
    "\n",
    "\n",
    "logit_pipe = Pipeline([\n",
    "    (\"preprocess\", logit_preprocess),\n",
    "    (\"logit\", LogisticRegression(max_iter=2000))\n",
    "])\n",
    "\n",
    "tree_pipe = Pipeline([\n",
    "    (\"preprocess\", tree_preprocess),\n",
    "    ('tree', DecisionTreeClassifier())\n",
    "])\n",
    "\n",
    "xgb_pipe = Pipeline([\n",
    "    (\"preprocess\", tree_preprocess),\n",
    "    (\"xgb\", XGBClassifier(random_state=21))\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97927fb-5aa0-4f09-a458-b53c11e567c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data\n",
    "X = df_rating[simplex_cols + cat_cols + num_cols]\n",
    "y = df_rating['above_average']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e68815-4077-4e11-89ab-dc7163203579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decision tree model hyperparameter tuning\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    \"tree__max_depth\": [None, 3, 5, 10],\n",
    "    \"tree__min_samples_leaf\": [1, 5, 10, 20, 30],\n",
    "    \"tree__min_samples_split\": [2, 5, 10]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    tree_pipe,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    scoring=\"accuracy\",\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "grid.best_params_, grid.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02e2074-e277-4fc5-a871-b5ec554d51f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tune the hyperparameters\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "xgb_param_grid = {\n",
    "    \"xgb__n_estimators\": [100, 300],\n",
    "    \"xgb__max_depth\": [3, 5],\n",
    "    \"xgb__learning_rate\": [0.05, 0.1],\n",
    "}\n",
    "\n",
    "xgb_gs = GridSearchCV(\n",
    "    xgb_pipe,\n",
    "    param_grid=xgb_param_grid,\n",
    "    cv=5,\n",
    "    scoring=\"accuracy\",   # or whatever you’re using\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "xgb_gs.fit(X_train, y_train)\n",
    "\n",
    "xgb_gs.best_params_, xgb_gs.best_score_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c810a6b-a5aa-46e4-95dd-c2fdb3c2ee57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# redefine tree_pipe\n",
    "\n",
    "best_tree_pipe = grid.best_estimator_\n",
    "\n",
    "best_xgb_pipe = xgb_gs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588e2b5e-7e7c-45c6-b916-1e8cc15d53b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep pipes in a list\n",
    "\n",
    "model_pipes_list = [(\"logit\", logit_pipe), (\"tree\", best_tree_pipe), ('xgboost', best_xgb_pipe)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd66ae7d-78e6-40ae-a454-c90bda56c2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline\n",
    "\n",
    "score_table = []\n",
    "\n",
    "for name, pipe in model_pipes_list:\n",
    "    scores = cross_val_score(\n",
    "        pipe,\n",
    "        X_train,\n",
    "        y_train,\n",
    "        cv=25,\n",
    "        scoring=\"accuracy\"   \n",
    "    )\n",
    "    \n",
    "    score_table.append({\n",
    "        \"model\": name,\n",
    "        \"mean_train_score\": scores.mean(),\n",
    "        \"cv_scores\": scores\n",
    "    })\n",
    "\n",
    "pd.DataFrame(score_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e670d3-55db-4840-9734-8d079cf59129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test scores\n",
    "\n",
    "test_scores = []\n",
    "\n",
    "for name, pipe in model_pipes_list:\n",
    "    \n",
    "    pipe.fit(X_train, y_train)\n",
    "    \n",
    "    test_scores.append({\n",
    "        \"model\" : name,\n",
    "        \"num_features\" : len(pipe.named_steps['preprocess'].get_feature_names_out()),\n",
    "        \"test_score\" : pipe.score(X_test, y_test),\n",
    "        \n",
    "    })\n",
    "    \n",
    "pd.DataFrame(test_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543a15e9-bb27-4ab1-8584-e07721ba9cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, pipe in model_pipes_list:\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    \n",
    "    # Column names from that pipeline's own preprocess step\n",
    "    ct = pipe.named_steps[\"preprocess\"]\n",
    "    feature_names = ct.get_feature_names_out()\n",
    "    print(\"n_features (names):\", len(feature_names))\n",
    "    \n",
    "    if name == \"logit\":\n",
    "        est = pipe.named_steps[\"logit\"]\n",
    "        importances = est.coef_.ravel()\n",
    "    elif name == \"tree\":\n",
    "        est = pipe.named_steps[\"tree\"]\n",
    "        importances = est.feature_importances_\n",
    "    elif name == \"xgboost\":\n",
    "        # make sure this matches the actual step name in the pipe!\n",
    "        est = pipe.named_steps[\"xgb\"]\n",
    "        importances = est.feature_importances_\n",
    "    else:\n",
    "        continue\n",
    "    \n",
    "    print(\"n_features (importances):\", importances.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1366e3-e710-4d87-82c4-20c9bdc07d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature importances per model\n",
    "\n",
    "rows = []\n",
    "\n",
    "for name, pipe in model_pipes_list:\n",
    "    # get this pipeline's feature names\n",
    "    ct = pipe.named_steps[\"preprocess\"]\n",
    "    feature_names = ct.get_feature_names_out()\n",
    "\n",
    "    # get this pipeline's importances\n",
    "    if name == \"logit\":\n",
    "        est = pipe.named_steps[\"logit\"]\n",
    "        importances = est.coef_.ravel()\n",
    "    elif name == \"tree\":\n",
    "        est = pipe.named_steps[\"tree\"]\n",
    "        importances = est.feature_importances_\n",
    "    elif name == \"xgboost\":\n",
    "        est = pipe.named_steps[\"xgb\"]\n",
    "        importances = est.feature_importances_\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "    # sanity check\n",
    "    assert len(feature_names) == importances.shape[0], f\"{name} mismatch\"\n",
    "\n",
    "    rows.append(\n",
    "        pd.DataFrame({\n",
    "            \"model\": name,\n",
    "            \"feature\": feature_names,\n",
    "            \"importance\": importances,\n",
    "            \"abs_fi\": np.abs(importances),\n",
    "        })\n",
    "    )\n",
    "\n",
    "fi_all = pd.concat(rows, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afcc297-7ab8-4158-8921-f6f8c23b8c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic feature importances\n",
    "\n",
    "rows =[]\n",
    "\n",
    "for name, model in model_pipes_list:\n",
    "    top_ten = fi_all[fi_all['model']==name].sort_values(by='abs_fi', ascending=False).head(10)\n",
    "    \n",
    "    display(top_ten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c5e5fc-9317-4229-b09c-685dc2f37820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot predictions versus truth\n",
    "def plot_classification_diagnostics_multi(models, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Plot ROC, Precision–Recall, and Calibration curves\n",
    "    for multiple classifiers on the same row of subplots.\n",
    "    \"\"\"\n",
    "\n",
    "    pos_rate = y_test.mean()\n",
    "\n",
    "    fig, ax = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "    # =========================\n",
    "    # 1. ROC curves\n",
    "    # =========================\n",
    "    for name, model in models:\n",
    "        y_proba = model.predict_proba(X_test)[:, 1]\n",
    "        fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        ax[0].plot(fpr, tpr, label=f\"{name} (AUC = {roc_auc:.3f})\")\n",
    "\n",
    "    ax[0].plot([0, 1], [0, 1], \"k--\", label=\"Random\")\n",
    "    ax[0].set_xlabel(\"False Positive Rate\")\n",
    "    ax[0].set_ylabel(\"True Positive Rate\")\n",
    "    ax[0].set_title(\"ROC Curve — All Models\")\n",
    "    ax[0].legend(loc=\"lower right\")\n",
    "    ax[0].grid(alpha=0.3)\n",
    "\n",
    "    # =========================\n",
    "    # 2. Precision–Recall curves\n",
    "    # =========================\n",
    "    for name, model in models:\n",
    "        y_proba = model.predict_proba(X_test)[:, 1]\n",
    "        precision, recall, _ = precision_recall_curve(y_test, y_proba)\n",
    "        ap = average_precision_score(y_test, y_proba)\n",
    "        ax[1].plot(recall, precision, label=f\"{name} (AP = {ap:.3f})\")\n",
    "\n",
    "    ax[1].hlines(pos_rate, 0, 1, linestyles=\"dashed\",\n",
    "                 label=f\"Baseline = {pos_rate:.2f}\")\n",
    "    ax[1].set_xlabel(\"Recall\")\n",
    "    ax[1].set_ylabel(\"Precision\")\n",
    "    ax[1].set_title(\"Precision–Recall Curve — All Models\")\n",
    "    ax[1].legend(loc=\"lower right\")\n",
    "    ax[1].grid(alpha=0.3)\n",
    "\n",
    "    # =========================\n",
    "    # 3. Calibration curves\n",
    "    # =========================\n",
    "    for name, model in models:\n",
    "        y_proba = model.predict_proba(X_test)[:, 1]\n",
    "        prob_true, prob_pred = calibration_curve(y_test, y_proba, n_bins=10)\n",
    "        ax[2].plot(prob_pred, prob_true, \"o-\", label=name)\n",
    "\n",
    "    ax[2].plot([0, 1], [0, 1], \"k--\", label=\"Perfect calibration\")\n",
    "    ax[2].set_xlabel(\"Predicted probability\")\n",
    "    ax[2].set_ylabel(\"Observed frequency\")\n",
    "    ax[2].set_title(\"Calibration Curve — All Models\")\n",
    "    ax[2].legend()\n",
    "    ax[2].grid(alpha=0.3)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.savefig('roc-pr-cal-curves.jpg')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f42fb1b-495d-4363-a63f-2a68331d54bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use function\n",
    "\n",
    "plot_classification_diagnostics_multi(model_pipes_list, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a421ac5-4777-49a8-bb13-7f6651f67957",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
